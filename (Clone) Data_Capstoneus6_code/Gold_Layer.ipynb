{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d798acec-f94d-4cdf-af66-3a411ad67cc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------\n",
    "# Gold Layer - Medallion Architecture (Service Principal Auth)\n",
    "# --------------------------------\n",
    "\n",
    "# Retrieve Service Principal credentials securely from Azure Key Vault via Databricks secrets\n",
    "client_id = dbutils.secrets.get(scope=\"secretscope_datacapus6\", key=\"client-id\")\n",
    "client_secret = dbutils.secrets.get(scope=\"secretscope_datacapus6\", key=\"client-secret\")\n",
    "tenant_id = dbutils.secrets.get(scope=\"secretscope_datacapus6\", key=\"tenant-id\")\n",
    "\n",
    "storage_account = \"storageaccus6\"\n",
    "silver_container = \"silver\"\n",
    "gold_container = \"gold\"\n",
    "\n",
    "# Configure Spark to use Service Principal OAuth for storage account\n",
    "spark.conf.set(\"fs.azure.account.auth.type\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id\", client_id)\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret\", client_secret)\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "\n",
    "# Base paths for Silver and Gold containers\n",
    "silver_base = f\"abfss://{silver_container}@{storage_account}.dfs.core.windows.net/\"\n",
    "gold_base = f\"abfss://{gold_container}@{storage_account}.dfs.core.windows.net/\"\n",
    "\n",
    "# Paths to Silver Delta tables\n",
    "silver_house_path = f\"{silver_base}house_price\"\n",
    "silver_orders_path = f\"{silver_base}sales_products\"\n",
    "silver_world_path = f\"{silver_base}population\"\n",
    "\n",
    "# Paths to Gold Delta tables\n",
    "gold_house_path = f\"{gold_base}house_price_summary\"\n",
    "gold_orders_path = f\"{gold_base}sales_orders_summary\"\n",
    "gold_world_path = f\"{gold_base}world_population_summary\"\n",
    "\n",
    "# --------------------------------\n",
    "# Read Silver Delta tables\n",
    "# --------------------------------\n",
    "df_house_silver = spark.read.format(\"delta\").load(silver_house_path)\n",
    "df_orders_silver = spark.read.format(\"delta\").load(silver_orders_path)\n",
    "df_world_silver = spark.read.format(\"delta\").load(silver_world_path)\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# --------------------------------\n",
    "# Gold Transformations (Business KPIs)\n",
    "# --------------------------------\n",
    "\n",
    "df_house_gold = (\n",
    "    df_house_silver\n",
    "    .withColumn(\"json_length\", F.length(F.col(\"value_clean\")))\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"total_records\"),\n",
    "        F.avg(\"json_length\").alias(\"avg_record_length\")\n",
    "    )\n",
    "    .withColumn(\"processed_at\", F.current_timestamp())\n",
    ")\n",
    "\n",
    "df_orders_gold = (\n",
    "    df_orders_silver\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"total_orders\"),\n",
    "        F.countDistinct(\"value_clean\").alias(\"unique_orders\")\n",
    "    )\n",
    "    .withColumn(\"processed_at\", F.current_timestamp())\n",
    ")\n",
    "\n",
    "df_world_gold = (\n",
    "    df_world_silver\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"total_population_records\"),\n",
    "        F.countDistinct(\"value_clean\").alias(\"unique_population_entries\")\n",
    "    )\n",
    "    .withColumn(\"processed_at\", F.current_timestamp())\n",
    ")\n",
    "\n",
    "# --------------------------------\n",
    "# Write Gold Delta tables\n",
    "# --------------------------------\n",
    "df_house_gold.write.format(\"delta\").mode(\"overwrite\").save(gold_house_path)\n",
    "df_orders_gold.write.format(\"delta\").mode(\"overwrite\").save(gold_orders_path)\n",
    "df_world_gold.write.format(\"delta\").mode(\"overwrite\").save(gold_world_path)\n",
    "\n",
    "# --------------------------------\n",
    "# Delta Lake Time Travel (Gold)\n",
    "# --------------------------------\n",
    "\n",
    "df_house_gold_v0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(gold_house_path)\n",
    "df_orders_gold_v0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(gold_orders_path)\n",
    "df_world_gold_v0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(gold_world_path)\n",
    "\n",
    "print(\"Gold House Price Summary (version 0):\")\n",
    "display(df_house_gold_v0)\n",
    "\n",
    "print(\"Gold Sales Orders Summary (version 0):\")\n",
    "display(df_orders_gold_v0)\n",
    "\n",
    "print(\"Gold World Population Summary (version 0):\")\n",
    "display(df_world_gold_v0)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Gold_Layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
