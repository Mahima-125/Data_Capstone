{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbe65b3b-45b7-4ca9-bc1d-ee7c9fc65d80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------\n",
    "# Silver Layer - Medallion Architecture (Service Principal Auth)\n",
    "# --------------------------------\n",
    "\n",
    "# Retrieve Service Principal credentials securely from Azure Key Vault via Databricks secrets\n",
    "client_id = dbutils.secrets.get(scope=\"secretscope_datacapus6\", key=\"client-id\")\n",
    "client_secret = dbutils.secrets.get(scope=\"secretscope_datacapus6\", key=\"client-secret\")\n",
    "tenant_id = dbutils.secrets.get(scope=\"secretscope_datacapus6\", key=\"tenant-id\")\n",
    "\n",
    "storage_account = \"storageaccus6\"\n",
    "bronze_container = \"bronze\"\n",
    "silver_container = \"silver\"\n",
    "\n",
    "# Configure Spark to use Service Principal OAuth for storage account\n",
    "spark.conf.set(\"fs.azure.account.auth.type\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id\", client_id)\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret\", client_secret)\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "\n",
    "# Base paths for Bronze and Silver containers\n",
    "bronze_base = f\"abfss://{bronze_container}@{storage_account}.dfs.core.windows.net/\"\n",
    "silver_base = f\"abfss://{silver_container}@{storage_account}.dfs.core.windows.net/\"\n",
    "\n",
    "# Paths to Bronze Delta tables\n",
    "bronze_house_path = f\"{bronze_base}house_price\"\n",
    "bronze_orders_path = f\"{bronze_base}sales_products\"\n",
    "bronze_world_path = f\"{bronze_base}population\"\n",
    "\n",
    "# Paths to Silver Delta tables\n",
    "silver_house_path = f\"{silver_base}house_price\"\n",
    "silver_orders_path = f\"{silver_base}sales_products\"\n",
    "silver_world_path = f\"{silver_base}population\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06f5b9b3-4fb2-4ffe-852c-fa376396c42a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Read Bronze Delta tables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "211484cd-9aca-4efb-bf6c-cc037ead60f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_house_bronze = spark.read.format(\"delta\").load(bronze_house_path)\n",
    "df_orders_bronze = spark.read.format(\"delta\").load(bronze_orders_path)\n",
    "df_world_bronze = spark.read.format(\"delta\").load(bronze_world_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fc4efcc-0d14-4566-bfc2-b32849fcbdaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Silver Layer Transformations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7c21d4a-b92c-465e-b885-29f086ab653a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------\n",
    "# Silver transformations (light cleaning, structuring, metadata)\n",
    "# --------------------------------\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import MapType, StringType\n",
    "\n",
    "def add_metadata(df, source_name):\n",
    "    return (\n",
    "        df\n",
    "        .withColumn(\"ingestion_timestamp\", F.current_timestamp())\n",
    "        .withColumn(\"ingestion_date\", F.to_date(F.current_timestamp()))\n",
    "        .withColumn(\"source_file\", F.input_file_name())\n",
    "        .withColumn(\"record_source\", F.lit(source_name))\n",
    "    )\n",
    "\n",
    "# House Price transformation\n",
    "df_house_silver = (\n",
    "    df_house_bronze\n",
    "    .withColumn(\"value_clean\", F.trim(F.col(\"value\")))\n",
    "    .filter(F.col(\"value_clean\").isNotNull() & (F.length(F.col(\"value_clean\")) > 0))\n",
    "    .withColumn(\"json_map\", F.from_json(F.col(\"value_clean\"), MapType(StringType(), StringType())))\n",
    "    .dropDuplicates([\"value_clean\"])\n",
    ")\n",
    "df_house_silver = add_metadata(df_house_silver, \"bronze.house_price\")\n",
    "df_house_silver = df_house_silver.withColumn(\"is_json\", F.col(\"json_map\").isNotNull())\n",
    "\n",
    "# Sales Orders transformation\n",
    "df_orders_silver = (\n",
    "    df_orders_bronze\n",
    "    .withColumn(\"value_clean\", F.trim(F.col(\"value\")))\n",
    "    .filter(F.col(\"value_clean\").isNotNull() & (F.length(F.col(\"value_clean\")) > 0))\n",
    "    .dropDuplicates([\"value_clean\"])\n",
    ")\n",
    "df_orders_silver = add_metadata(df_orders_silver, \"bronze.sales_orders\")\n",
    "\n",
    "# World Population transformation\n",
    "df_world_silver = (\n",
    "    df_world_bronze\n",
    "    .withColumn(\"value_clean\", F.trim(F.col(\"value\")))\n",
    "    .filter(F.col(\"value_clean\").isNotNull() & (F.length(F.col(\"value_clean\")) > 0))\n",
    "    .dropDuplicates([\"value_clean\"])\n",
    ")\n",
    "df_world_silver = add_metadata(df_world_silver, \"bronze.world_population\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "748f2d19-d01a-4658-9afa-4d7418cb5e01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------\n",
    "# Write Silver Delta tables\n",
    "# --------------------------------\n",
    "(\n",
    "    df_house_silver.write.format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .save(silver_house_path)\n",
    ")\n",
    "\n",
    "(\n",
    "    df_orders_silver.write.format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .save(silver_orders_path)\n",
    ")\n",
    "\n",
    "(\n",
    "    df_world_silver.write.format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .save(silver_world_path)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9da76e4-91c3-4267-978b-8c692d46daa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------------\n",
    "# Step 4: Delta Lake Time Travel Examples (Silver Layer)\n",
    "# --------------------------------\n",
    "# Read version 0 of each Silver table\n",
    "df_house_silver_v0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(silver_house_path)\n",
    "df_orders_silver_v0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(silver_orders_path)\n",
    "df_world_silver_v0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(silver_world_path)\n",
    "\n",
    "print(\"Silver House Price (version 0):\")\n",
    "display(df_house_silver_v0.limit(5))\n",
    "\n",
    "print(\"Silver Sales Orders (version 0):\")\n",
    "display(df_orders_silver_v0.limit(5))\n",
    "\n",
    "print(\"Silver World Population (version 0):\")\n",
    "display(df_world_silver_v0.limit(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "556e2351-330d-478d-a6ad-3256f6a130d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------\n",
    "# Delta Lake time travel examples (Silver)\n",
    "# --------------------------------\n",
    "df_house_silver_v0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(silver_house_path)\n",
    "df_orders_silver_v0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(silver_orders_path)\n",
    "df_world_silver_v0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(silver_world_path)\n",
    "\n",
    "print(\"Silver House Price (version 0):\")\n",
    "display(df_house_silver_v0.limit(5))\n",
    "\n",
    "print(\"Silver Sales Orders (version 0):\")\n",
    "display(df_orders_silver_v0.limit(5))\n",
    "\n",
    "print(\"Silver World Population (version 0):\")\n",
    "display(df_world_silver_v0.limit(5))\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "house_delta = DeltaTable.forPath(spark, silver_house_path)\n",
    "orders_delta = DeltaTable.forPath(spark, silver_orders_path)\n",
    "world_delta = DeltaTable.forPath(spark, silver_world_path)\n",
    "\n",
    "house_first_ts = house_delta.history().orderBy(F.col(\"timestamp\").asc()).first()[\"timestamp\"]\n",
    "orders_first_ts = orders_delta.history().orderBy(F.col(\"timestamp\").asc()).first()[\"timestamp\"]\n",
    "world_first_ts = world_delta.history().orderBy(F.col(\"timestamp\").asc()).first()[\"timestamp\"]\n",
    "\n",
    "df_house_silver_t0 = spark.read.format(\"delta\").option(\"timestampAsOf\", str(house_first_ts)).load(silver_house_path)\n",
    "df_orders_silver_t0 = spark.read.format(\"delta\").option(\"timestampAsOf\", str(orders_first_ts)).load(silver_orders_path)\n",
    "df_world_silver_t0 = spark.read.format(\"delta\").option(\"timestampAsOf\", str(world_first_ts)).load(silver_world_path)\n",
    "\n",
    "print(f\"Silver House Price (timestampAsOf={house_first_ts}):\")\n",
    "display(df_house_silver_t0.limit(5))\n",
    "\n",
    "print(f\"Silver Sales Orders (timestampAsOf={orders_first_ts}):\")\n",
    "display(df_orders_silver_t0.limit(5))\n",
    "\n",
    "print(f\"Silver World Population (timestampAsOf={world_first_ts}):\")\n",
    "display(df_world_silver_t0.limit(5))\n",
    "\n",
    "print(\"Silver House Price history:\")\n",
    "house_delta.history().show(truncate=False)\n",
    "\n",
    "print(\"Silver Sales Orders history:\")\n",
    "orders_delta.history().show(truncate=False)\n",
    "\n",
    "print(\"Silver World Population history:\")\n",
    "world_delta.history().show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcfbe191-41e5-468f-a909-36452ba0b1a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------\n",
    "# Register Silver tables in SQL database\n",
    "# --------------------------------\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS silver_db\")\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS silver_db.house_price USING DELTA LOCATION '{silver_house_path}'\")\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS silver_db.sales_orders USING DELTA LOCATION '{silver_orders_path}'\")\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS silver_db.world_population USING DELTA LOCATION '{silver_world_path}'\")\n",
    "\n",
    "# Sample query\n",
    "display(spark.sql(\"SELECT * FROM silver_db.house_price LIMIT 5\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver_Layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
